{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KbIxGfMDRdEW"
      },
      "outputs": [],
      "source": [
        "#@markdown ### RUN GPU\n",
        "\n",
        "! nvidia-smi\n",
        "! nvcc -V\n",
        "! free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FnbJY5YxLmaG"
      },
      "outputs": [],
      "source": [
        "#@markdown ### install ollama in google colab (not google drive)\n",
        "\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y curl wget\n",
        "\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "\n",
        "!ollama --version\n",
        "\n",
        "\n",
        "!mkdir -p /tmp/ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "euYh88sIL_6F"
      },
      "outputs": [],
      "source": [
        "#@markdown ### run ollama\n",
        "\n",
        "# Set environment variables for Ollama to store models in temporary memory\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "os.environ[\"OLLAMA_MODELS\"] = \"/tmp/ollama\"\n",
        "\n",
        "# Launch Ollama service in the background using subprocess\n",
        "with open('/tmp/ollama.log', 'w') as log_file:\n",
        "    process = subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=log_file,\n",
        "        stderr=log_file,\n",
        "        preexec_fn=os.setsid  # For execution in a new process group\n",
        "    )\n",
        "\n",
        "# Wait a bit for the service to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Check service status\n",
        "try:\n",
        "    # Try to run a simple ollama command to confirm that the service is running\n",
        "    check_process = subprocess.run(\n",
        "        [\"ollama\", \"list\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True,\n",
        "        timeout=10\n",
        "    )\n",
        "    if check_process.returncode == 0:\n",
        "        print(\"‚úÖ Ollama service has been successfully launched!\")\n",
        "        print(\"\\nLatest log lines:\")\n",
        "        !tail -n 5 /tmp/ollama.log\n",
        "    else:\n",
        "        print(\"‚ùå There is a problem launching Ollama:\")\n",
        "        print(check_process.stderr)\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ùå Response time has expired. The service may still be starting.\")\n",
        "    print(\"\\nLatest log lines:\")\n",
        "    !tail -n 5 /tmp/ollama.log\n",
        "\n",
        "print(\"\\nüîç Ollama process ID:\")\n",
        "!pgrep -f \"ollama serve\"\n",
        "\n",
        "# Display the address and port for accessing the service\n",
        "print(\"\\nüåê Ollama service is available at the following address:\")\n",
        "print(\"http://127.0.0.1:11434\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yO3wQtLJNaEl"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Download ollama models\n",
        "\n",
        "#@markdown ##### 8B parameters (~4-5GB disk space)\n",
        "deepseek_r1_8b = False #@param {type:\"boolean\"}\n",
        "llama3_8b = False #@param {type:\"boolean\"}\n",
        "qwen3_8b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 8B parameters, 128K context (~4-5GB disk space)\n",
        "llama3_1_8b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 7B parameters (~4GB disk space)\n",
        "mistral_7b = False #@param {type:\"boolean\"}\n",
        "gemma_7b = False #@param {type:\"boolean\"}\n",
        "falcon_7b = False #@param {type:\"boolean\"}\n",
        "codegemma_7b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 14B parameters (~7-8GB disk space)\n",
        "deepseek_r1_14b = False #@param {type:\"boolean\"}\n",
        "qwen3_14b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 12B parameters, 128K context (~7GB disk space)\n",
        "gemma3_12b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 32B parameters (~16-18GB disk space)\n",
        "deepseek_r1_32b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 27B parameters, 128K context (~14-15GB disk space)\n",
        "gemma3_27b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 30.5B parameters (~16GB disk space)\n",
        "qwen3_30b = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ##### 3.8B parameters (~2GB disk space)\n",
        "phi3_mini = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Add custom models\n",
        "#@markdown Enter custom model names in the format `model1:tag1,model2:tag2`. Example: `vicuna:latest,orca-mini:3b`\n",
        "custom_models = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Test prompt settings\n",
        "#@markdown Enter a test prompt to evaluate models after download\n",
        "run_test_prompt = False #@param {type:\"boolean\"}\n",
        "test_prompt = \"surprise me\" #@param {type:\"string\"}\n",
        "\n",
        "# Dictionary of predefined models and their selection status\n",
        "models = {\n",
        "    \"deepseek:r1-8b\": deepseek_r1_8b,\n",
        "    \"llama3:8b\": llama3_8b,\n",
        "    \"qwen3:8b\": qwen3_8b,\n",
        "    \"llama3.1:8b\": llama3_1_8b,\n",
        "    \"mistral:7b\": mistral_7b,\n",
        "    \"gemma:7b\": gemma_7b,\n",
        "    \"falcon:7b\": falcon_7b,\n",
        "    \"codegemma:7b\": codegemma_7b,\n",
        "    \"deepseek-r1:14b\": deepseek_r1_14b,\n",
        "    \"qwen3:14b\": qwen3_14b,\n",
        "    \"gemma3:12b\": gemma3_12b,\n",
        "    \"deepseek-r1:32b\": deepseek_r1_32b,\n",
        "    \"gemma3:27b\": gemma3_27b,\n",
        "    \"qwen3:30b\": qwen3_30b,\n",
        "    \"phi3:mini\": phi3_mini\n",
        "}\n",
        "\n",
        "# Model size information for display\n",
        "model_sizes = {\n",
        "    \"deepseek:r1-8b\": \"~8B parameters (~4-5GB disk space)\",\n",
        "    \"llama3:8b\": \"~8B parameters (~4-5GB disk space)\",\n",
        "    \"qwen3:8b\": \"~8.19B parameters (~5GB disk space)\",\n",
        "    \"llama3.1:8b\": \"~8B parameters, 128K context (~4-5GB disk space)\",\n",
        "    \"mistral:7b\": \"~7B parameters (~4GB disk space)\",\n",
        "    \"gemma:7b\": \"~7B parameters (~4GB disk space)\",\n",
        "    \"falcon:7b\": \"~7B parameters (~4GB disk space)\",\n",
        "    \"codegemma:7b\": \"~7B parameters (~4GB disk space)\",\n",
        "    \"deepseek-r1:14b\": \"~14B parameters (~7-8GB disk space)\",\n",
        "    \"qwen3:14b\": \"~14.8B parameters (~8GB disk space)\",\n",
        "    \"gemma3:12b\": \"~12B parameters, 128K context (~7GB disk space)\",\n",
        "    \"deepseek-r1:32b\": \"~32B parameters (~16-18GB disk space)\",\n",
        "    \"gemma3:27b\": \"~27B parameters, 128K context (~14-15GB disk space)\",\n",
        "    \"qwen3:30b\": \"~30.5B parameters (~16GB disk space)\",\n",
        "    \"phi3:mini\": \"~3.8B parameters (~2GB disk space)\"\n",
        "}\n",
        "\n",
        "# Keep track of successfully downloaded models\n",
        "downloaded_model_list = []\n",
        "\n",
        "# Add custom models to the dictionary\n",
        "if custom_models.strip():\n",
        "    # Split input text by commas\n",
        "    custom_model_list = [model.strip() for model in custom_models.split(',') if model.strip()]\n",
        "\n",
        "    # Add each custom model to the dictionary with selection status True\n",
        "    for model_name in custom_model_list:\n",
        "        models[model_name] = True\n",
        "        print(f\"Custom model '{model_name}' added to download list.\")\n",
        "\n",
        "# Display model information and download selected models\n",
        "downloaded_models = 0\n",
        "print(\"\\n### MODEL INFORMATION ###\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'MODEL NAME':<20} {'SELECTION':<15} {'SIZE'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for model_name, selected in models.items():\n",
        "    size_info = model_sizes.get(model_name, \"Size information not available\")\n",
        "    status = \"SELECTED\" if selected else \"Not selected\"\n",
        "    print(f\"{model_name:<20} {status:<15} {size_info}\")\n",
        "\n",
        "print(\"\\n### DOWNLOADING SELECTED MODELS ###\")\n",
        "for model_name, selected in models.items():\n",
        "    if selected:\n",
        "        print(f\"\\nDownloading model {model_name}...\")\n",
        "        !ollama pull {model_name}\n",
        "        download_status = !echo $?\n",
        "\n",
        "        if int(download_status[0]) == 0:\n",
        "            print(f\"Model {model_name} downloaded successfully.\")\n",
        "            downloaded_models += 1\n",
        "            downloaded_model_list.append(model_name)\n",
        "        else:\n",
        "            print(f\"Error downloading model {model_name}.\")\n",
        "\n",
        "if downloaded_models == 0:\n",
        "    print(\"\\n‚ö†Ô∏è No models selected for download!\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ {downloaded_models} models downloaded successfully.\")\n",
        "\n",
        "print(\"\\n### List of downloaded models ###\")\n",
        "!ollama list\n",
        "\n",
        "# Run test prompt if enabled and models were downloaded\n",
        "if run_test_prompt and downloaded_model_list:\n",
        "    import time\n",
        "    import json\n",
        "    import subprocess\n",
        "    import re\n",
        "    from datetime import datetime\n",
        "    from IPython.display import display, HTML, Markdown\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"#\" * 80)\n",
        "    print(f\"### RUNNING TEST PROMPT: '{test_prompt}' ###\")\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    def clean_ansi_codes(text):\n",
        "        \"\"\"Remove ANSI escape codes from text\"\"\"\n",
        "        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
        "        return ansi_escape.sub('', text)\n",
        "\n",
        "    for model_name in downloaded_model_list:\n",
        "        print(f\"\\n\\n## Testing model: {model_name} ##\\n\")\n",
        "\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Run the model with direct subprocess call for better control\n",
        "        print(\"Generating response...\")\n",
        "        try:\n",
        "            # Run without --format json to avoid the formatting issues\n",
        "            process = subprocess.Popen(\n",
        "                [\"ollama\", \"run\", model_name, test_prompt],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            model_response, stderr = process.communicate()\n",
        "\n",
        "            # Clean response\n",
        "            model_response = clean_ansi_codes(model_response).strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            model_response = f\"Error running model: {str(e)}\"\n",
        "\n",
        "        # Record end time and calculate duration\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            \"response\": model_response,\n",
        "            \"time\": duration\n",
        "        }\n",
        "\n",
        "        # Display the response\n",
        "        print(f\"\\nResponse from {model_name} (took {duration:.2f} seconds):\")\n",
        "        print(\"-\" * 60)\n",
        "        display(Markdown(model_response))\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # Comparison summary\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"#\" * 80)\n",
        "    print(\"### MODEL COMPARISON SUMMARY ###\")\n",
        "    print(\"#\" * 80)\n",
        "\n",
        "    print(f\"\\n{'MODEL':<20} {'RESPONSE TIME':<15} {'RESPONSE LENGTH'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for model, data in results.items():\n",
        "        response_length = len(data[\"response\"])\n",
        "        print(f\"{model:<20} {data['time']:.2f}s{' ':<10} {response_length} chars\")\n",
        "\n",
        "    # Save results to file\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    result_file = f\"model_comparison_{timestamp}.txt\"\n",
        "\n",
        "    with open(result_file, \"w\") as f:\n",
        "        f.write(f\"TEST PROMPT: {test_prompt}\\n\")\n",
        "        f.write(f\"DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "        for model, data in results.items():\n",
        "            f.write(f\"MODEL: {model}\\n\")\n",
        "            f.write(f\"TIME: {data['time']:.2f}s\\n\")\n",
        "            f.write(f\"RESPONSE LENGTH: {len(data['response'])} chars\\n\")\n",
        "            f.write(\"-\" * 60 + \"\\n\")\n",
        "            f.write(data[\"response\"])\n",
        "            f.write(\"\\n\\n\" + \"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\nDetailed comparison saved to '{result_file}'\")\n",
        "elif run_test_prompt and not downloaded_model_list:\n",
        "    print(\"\\n‚ö†Ô∏è No models were successfully downloaded to test.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8dPXWnxzWoNW"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Install N*N\n",
        "\n",
        "# Input fields for parameter configuration\n",
        "\n",
        "# Enter your ngrok authentication token\n",
        "NGROK_AUTH_TOKEN = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Storage folder name in Google Drive\n",
        "gdrive_path = \"n8n_data\" #@param {type:\"string\"}\n",
        "# Authentication settings\n",
        "N8N_BASIC_AUTH = False\n",
        "N8N_USERNAME = \"admin\"\n",
        "N8N_PASSWORD = \"password\"\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok, conf\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Connect to Google Drive for persistent storage\n",
        "from google.colab import drive\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up storage path in Google Drive for n8n data\n",
        "import os\n",
        "\n",
        "# Define full Google Drive path\n",
        "gdrive_full_path = f\"/content/drive/MyDrive/{gdrive_path}\"\n",
        "\n",
        "# Create folders if they don't exist\n",
        "if not os.path.exists(gdrive_full_path):\n",
        "    os.makedirs(gdrive_full_path)\n",
        "    print(f\"Folder '{gdrive_path}' created in Google Drive.\")\n",
        "else:\n",
        "    print(f\"Folder '{gdrive_path}' already exists in Google Drive.\")\n",
        "\n",
        "# Create necessary subfolders\n",
        "os.makedirs(f\"{gdrive_full_path}/data\", exist_ok=True)\n",
        "os.makedirs(f\"{gdrive_full_path}/database\", exist_ok=True)\n",
        "print(\"Storage folders prepared.\")\n",
        "\n",
        "# Set up ngrok authentication\n",
        "# Validate ngrok token\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"‚ö†Ô∏è ngrok authentication token not entered! Please get your token from https://dashboard.ngrok.com/get-started/your-authtoken.\")\n",
        "    raise ValueError(\"ngrok authentication token is required.\")\n",
        "\n",
        "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "print(\"ngrok configured successfully.\")\n",
        "\n",
        "# ===================================================================\n",
        "# Check Node.js version and install if needed\n",
        "# ===================================================================\n",
        "def get_node_version():\n",
        "    try:\n",
        "        result = subprocess.run(['node', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if result.returncode == 0:\n",
        "            version = result.stdout.decode('utf-8').strip()\n",
        "            return version\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def is_n8n_installed():\n",
        "    try:\n",
        "        result = subprocess.run(['which', 'n8n'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Check Node.js version\n",
        "node_version = get_node_version()\n",
        "if node_version:\n",
        "    print(f\"Node.js installed: {node_version}\")\n",
        "    # Check if current version is new enough\n",
        "    if not node_version.startswith('v18.') and not node_version.startswith('v19.') and not node_version.startswith('v20.'):\n",
        "        print(\"Installed Node.js version is not sufficient, installing version 18...\")\n",
        "        !curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
        "        !sudo apt-get install -y nodejs\n",
        "        print(\"Node.js updated:\")\n",
        "        !node --version\n",
        "else:\n",
        "    print(\"Node.js not installed, installing...\")\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -\n",
        "    !sudo apt-get install -y nodejs\n",
        "    print(\"Node.js installed:\")\n",
        "    !node --version\n",
        "\n",
        "# Check if n8n is already installed\n",
        "if is_n8n_installed():\n",
        "    print(\"n8n is already installed, no need to reinstall.\")\n",
        "    !n8n --version\n",
        "else:\n",
        "    print(\"n8n not installed, installing...\")\n",
        "    !npm install n8n -g\n",
        "    print(\"n8n installed.\")\n",
        "# ===================================================================\n",
        "\n",
        "# Create n8n configuration file\n",
        "n8n_config = f\"\"\"\n",
        "module.exports = {{\n",
        "  database: {{\n",
        "    type: 'sqlite',\n",
        "    logging: false,\n",
        "    sqliteOptions: {{\n",
        "      path: '{gdrive_full_path}/database/database.sqlite',\n",
        "    }},\n",
        "  }},\n",
        "  executions: {{\n",
        "    saveDataOnError: 'all',\n",
        "    saveDataOnSuccess: 'all',\n",
        "    saveExecutionProgress: true,\n",
        "    saveManualExecutions: true,\n",
        "  }},\n",
        "\"\"\"\n",
        "\n",
        "# Add authentication configuration if enabled\n",
        "if N8N_BASIC_AUTH:\n",
        "    n8n_config += f\"\"\"\n",
        "  basic: {{\n",
        "    auth: {{\n",
        "      active: true,\n",
        "      user: '{N8N_USERNAME}',\n",
        "      password: '{N8N_PASSWORD}',\n",
        "    }},\n",
        "  }},\n",
        "\"\"\"\n",
        "\n",
        "n8n_config += \"\"\"\n",
        "  userManagement: {\n",
        "    isInstanceOwnerSetUp: true,\n",
        "  },\n",
        "  diagnostics: {\n",
        "    enabled: false,\n",
        "  },\n",
        "};\n",
        "\"\"\"\n",
        "\n",
        "# Write n8n configuration file\n",
        "config_dir = f\"{gdrive_full_path}/config\"\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "with open(f\"{config_dir}/config.js\", 'w') as f:\n",
        "    f.write(n8n_config)\n",
        "\n",
        "# Create symbolic link to n8n configuration in home folder\n",
        "!mkdir -p ~/.n8n\n",
        "!ln -sf {config_dir}/config.js ~/.n8n/config.js\n",
        "\n",
        "# Start ngrok to get public URL before starting n8n\n",
        "print(\"Starting ngrok tunnel...\")\n",
        "ngrok.kill()  # Kill existing tunnels\n",
        "tunnel = ngrok.connect(5678, 'http')\n",
        "public_url = tunnel.public_url\n",
        "print(f\"Tunnel address: {public_url}\")\n",
        "\n",
        "# Set environment variables for n8n\n",
        "os.environ[\"N8N_HOST\"] = \"localhost\"\n",
        "os.environ[\"N8N_PORT\"] = \"5678\"\n",
        "os.environ[\"N8N_PROTOCOL\"] = \"http\"\n",
        "os.environ[\"N8N_PATH\"] = \"/\"\n",
        "os.environ[\"NODE_ENV\"] = \"production\"\n",
        "os.environ[\"N8N_ENCRYPTION_KEY\"] = \"your-secret-encryption-key\"\n",
        "os.environ[\"N8N_USER_FOLDER\"] = f\"{gdrive_full_path}/data\"\n",
        "os.environ[\"WEBHOOK_URL\"] = public_url\n",
        "# Add this line to use the ngrok URL directly\n",
        "os.environ[\"N8N_WEBHOOK_URL\"] = public_url\n",
        "\n",
        "# Launch n8n in the background - REMOVED --tunnel flag\n",
        "print(\"Starting n8n...\")\n",
        "!nohup n8n start > {gdrive_full_path}/n8n.log 2>&1 &\n",
        "print(\"Waiting for n8n to start...\")\n",
        "time.sleep(30)\n",
        "\n",
        "# Check if n8n is running\n",
        "!ps aux | grep n8n\n",
        "\n",
        "# Display success message and access information\n",
        "import IPython.display as display\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"‚úÖ n8n launched successfully!\")\n",
        "print(\"-\"*50)\n",
        "print(f\"üåê Access n8n via: {public_url}\")\n",
        "\n",
        "if N8N_BASIC_AUTH:\n",
        "    print(f\"üë§ Username: {N8N_USERNAME}\")\n",
        "    print(f\"üîë Password: {N8N_PASSWORD}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"üöÄ Usage instructions:\")\n",
        "print(\"-\"*50)\n",
        "print(\"1Ô∏è‚É£ To use Ollama models in n8n, use an HTTP Request node.\")\n",
        "print(\"2Ô∏è‚É£ Ollama API address: http://localhost:11434/api/generate\")\n",
        "print(\"3Ô∏è‚É£ To test Ollama connection in Colab, you can run these commands:\")\n",
        "print(\"   !ollama list\")\n",
        "print(\"   !curl -X POST http://localhost:11434/api/generate -d '{\\\"model\\\":\\\"[model-name]\\\",\\\"prompt\\\":\\\"hello\\\"}'\")\n",
        "print(\"\\n4Ô∏è‚É£ To maintain access to n8n, keep this notebook open.\")\n",
        "print(\"5Ô∏è‚É£ For long-term usage, use strategies to prevent Colab from disconnecting.\")\n",
        "\n",
        "# Display n8n interface in an iframe\n",
        "display.display(display.IFrame(src=public_url, width=\"100%\", height=800))\n",
        "\n",
        "# Function to view logs\n",
        "def view_logs():\n",
        "    print(\"n8n logs:\")\n",
        "    !tail -n 20 {gdrive_full_path}/n8n.log\n",
        "    print(\"\\nOllama logs:\")\n",
        "    !tail -n 20 {gdrive_full_path}/ollama.log\n",
        "\n",
        "# Keep notebook running\n",
        "print(\"\\nNotebook is maintaining n8n and Ollama. Do not close this tab.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
